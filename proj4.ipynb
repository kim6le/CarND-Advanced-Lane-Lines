{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The related files are: \n",
    "* The writeup: this document you are reading now.\n",
    "* Python code: `proj4.py`\n",
    "* The generated video file: `result.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer: ** OpenCV equips functions to calculate the correct camera matrix and distortion coefficients. Twenty 9x6 chessboard images are provided for calibration. Note that some provided images are not recognized as 9x6 chessboard, therefore, I only used 17 of them. The distortion matrix is used to un-distort the calibration images. The undistorted calibration images are included in the next section for the purpose of demonstration that the calibration are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Camera Calibration ###########\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following function returns the (1) camera calibration matrix (mtx); (2) distortion coefficients (dist); and (3) \n",
    "# undistorted image (undist)\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    # Use cv2.calibrateCamera and cv2.undistort()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, (img.shape[1], img.shape[0]),None,None)\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    undist = np.copy(dst)  # Delete this line\n",
    "    return mtx, dist, undist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step through the list and search for chessboard corners\n",
    "for image in images: # Here 'image' means the path-name of an image\n",
    "    img = cv2.imread(image)\n",
    "    #img2 = np.copy(img)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    # There are 5 different sizes of the given calibration images\n",
    "    nx=9\n",
    "    ny=6\n",
    "    img_size=(nx,ny)\n",
    "    # chessboard size is (nx,xy)\n",
    "\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "\n",
    "    objp = np.zeros((ny*nx,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        print(image)\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        #cv2.drawChessboardCorners(img2, (ny,nx), corners, ret)\n",
    "        img2 = cv2.drawChessboardCorners(img, (nx,ny), corners, ret)\n",
    "        mtx, dist, undistorted = cal_undistort(img, objpoints, imgpoints)\n",
    "        # print(mtx) \n",
    "        # print(dist)\n",
    "    \n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(img2)\n",
    "        ax1.set_title(image+'\\n'+'Original Image with Corners', fontsize=24)\n",
    "        ax2.imshow(undistorted)\n",
    "        ax2.set_title('Undistorted Image', fontsize=24)\n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distortion Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Provide an example of a distortion-corrected image. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer:** In this section, I apply the camera matrix and distortion coefficients obtained in the previous section, to the test images. Distortion-corrected images are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a list of raw images\n",
    "images = glob.glob('test_images/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    # The image loaded by cv2 is BGR channel, should be transformed into RGB channels\n",
    "    \n",
    "    # The parameters of the following function (mtx and dist) is adopted from the previous step\n",
    "    undistorted = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # parameters: objpoints, imgpoints; are obtained in the first step\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image', fontsize=50)\n",
    "    ax2.imshow(undistorted)\n",
    "    ax2.set_title('Undistorted Image', fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Thresholded Binary Image Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. Provide an example of a binary image result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** In this section, I defined several functions to create binary images that contain lane pixels. \n",
    "* The function `abs_sobel_thresh`: This is the Sobel operator for either $x$ or $y$ direction. The Sobel operator is at the heart of the Canny edge detection algorithm. Applying the Sobel operator to an image is a way of taking the derivative of the image in the $x$ or $y$ direction.\n",
    "* The function `mag_thresh`: The magnitude, or absolute value, of the gradient is just the square root of the squares of the individual $x$ and $y$ gradients. For a gradient in both the $x$ and $y$ directions, the magnitude is the square root of the sum of the squares.\n",
    "* The function `dir_threshold`: The direction of the gradient is simply the inverse tangent (arctangent) of the $y$ gradient divided by the $x$ gradient.\n",
    "* The functions `hls_s` and `hls_h` perform color transfors.\n",
    "* The function `thresholded_binary` combines the above four operators. The combination method is obtained by trial and error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############  Thresholded Binary Image Creation ####################\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # 1) Convert to grayscale\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    if orient == 'x':\n",
    "        sobelxory=cv2.Sobel(gray,cv2.CV_64F,1,0)\n",
    "    elif orient == 'y':\n",
    "        sobelxory=cv2.Sobel(gray,cv2.CV_64F,0,1)\n",
    "    else:\n",
    "        print(\"orient should be 'x' or 'y'\")\n",
    "    # 3) Take the absolute value of the derivative or gradient\n",
    "    abs_sobelxory=np.absolute(sobelxory)\n",
    "    # 4) Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel=np.uint8((255*abs_sobelxory)/np.max(abs_sobelxory))\n",
    "    # 5) Create a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1    \n",
    "    # 6) Return this mask as your binary_output image\n",
    "    grad_binary = np.copy(sxbinary) \n",
    "    return grad_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mag_thresh(image,  sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # 1) Convert to grayscale\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx=cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=sobel_kernel)\n",
    "    sobely=cv2.Sobel(gray,cv2.CV_64F,0,1,ksize=sobel_kernel)\n",
    "    # 3) Calculate the magnitude \n",
    "    abs_sobelxy = np.sqrt(sobelx**2  + sobely**2)\n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scale_sobelxy=np.uint8((255*abs_sobelxy)/np.max(abs_sobelxy))\n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    binary_sobelxy = np.zeros_like(scale_sobelxy)\n",
    "    binary_sobelxy[(scale_sobelxy >= mag_thresh[0]) & (scale_sobelxy <= mag_thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    mag_binary = np.copy(binary_sobelxy)\n",
    "    return mag_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dir_threshold(image, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # 1) Convert to grayscale\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx=cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=sobel_kernel)\n",
    "    sobely=cv2.Sobel(gray,cv2.CV_64F,0,1,ksize=sobel_kernel)\n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobely = np.sqrt(sobely**2)\n",
    "    abs_sobelx = np.sqrt(sobelx**2)\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient\n",
    "    dir_grad=np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    binary_sobelxy = np.zeros_like(dir_grad)\n",
    "    binary_sobelxy[(dir_grad >= thresh[0]) & (dir_grad <= thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    dir_binary = np.copy(binary_sobelxy)\n",
    "    return dir_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that thresholds the S-channel of HLS\n",
    "def hls_s(img, thresh=(0, 255)):\n",
    "    # 1) Convert to HLS color space\n",
    "    # 2) Apply a threshold to the S channel\n",
    "    # 3) Return a binary image of threshold result\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    s = hls[:,:,2]\n",
    "    retval, s_binary = cv2.threshold(s.astype('uint8'), thresh[0], thresh[1], cv2.THRESH_BINARY)\n",
    "    return s_binary\n",
    "\n",
    "# Define a function that thresholds the S-channel of HLS\n",
    "def hls_h(img, thresh=(0, 255)):\n",
    "    # 1) Convert to HLS color space\n",
    "    # 2) Apply a threshold to the S channel\n",
    "    # 3) Return a binary image of threshold result\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    h = hls[:,:,0]\n",
    "    retval, h_binary = cv2.threshold(h.astype('uint8'), thresh[0], thresh[1], cv2.THRESH_BINARY)\n",
    "    return h_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thresholded_binary(img):\n",
    "    gradx = abs_sobel_thresh(img, orient='x', sobel_kernel=ksize, thresh=(20,100))\n",
    "    grady = abs_sobel_thresh(img, orient='y', sobel_kernel=ksize, thresh=(20,100))\n",
    "    mag_binary = mag_thresh(img, sobel_kernel=ksize, mag_thresh=(30,100))\n",
    "    dir_binary = dir_threshold(img, sobel_kernel=ksize, thresh=(0.7,1.3))\n",
    "    \n",
    "    sch = hls_s(img, thresh=(88, 190))\n",
    "    hch = hls_h(img, thresh=(50, 100))\n",
    "    \n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))|((sch>0) & (hch<=0))] = 1\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a Sobel kernel size\n",
    "ksize = 3 # Choose a larger odd number to smooth gradient measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes generate the thresholded binary images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a list of raw images\n",
    "images = glob.glob('test_images/*.jpg')\n",
    "for image in images:\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    \n",
    "    undistorted = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    combined=thresholded_binary(undistorted)\n",
    "    \n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image', fontsize=50)\n",
    "    ax2.imshow(combined,cmap='gray')\n",
    "    ax2.set_title('Thresholded Binary Image', fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Perspective Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "* I investigated an image where the lane lines are straight, and find four points lying along the lines that, after perspective transform, make the lines look straight and vertical from a bird's eye view perspective. That is, the `src` four points form a trapezoidal shape, and the `dst` four points form a rectangle.\n",
    "* I used OpenCV function, `cv2.getPerspectiveTransform(src,dst)`, to get transform matrix, and used `cv2.warpPerspective` to transform each image into a \"birds-eye view\". The inverse transform matrix could be obtained by `cv2.getPerspectiveTransform(dst,src)`. Transformed images are generated as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########  Perspective Transform ###################\n",
    "\n",
    "def perspective_transform(img):\n",
    "    \n",
    "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    ret1, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, (img.shape[1], img.shape[0]),None,None)\n",
    "        \n",
    "    img_size=(img.shape[1],img.shape[0])\n",
    "    # 1) Undistort using mtx and dist\n",
    "    dstimg = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # 2) Convert to grayscale\n",
    "    #gray = cv2.cvtColor(dstimg,cv2.COLOR_BGR2GRAY)\n",
    "    # 3) Find the chessboard corners\n",
    "\n",
    "    src = np.float32([[200,680],[582,450],[695,450],[1080,680]])\n",
    "    # c) define 4 destination points dst = np.float32([[,],[,],[,],[,]])\n",
    "    offset = 100 # offset for dst points\n",
    "    dst = np.float32([[200,710], [200,40], [1085,40],[1085,710]])\n",
    "    # d) use cv2.getPerspectiveTransform() to get M, the transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # e) use cv2.warpPerspective() to warp your image to a top-down view\n",
    "    warped = cv2.warpPerspective(dstimg, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    return warped, Minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "images = glob.glob('test_images/*.jpg')\n",
    "for image in images:\n",
    "    img = cv2.imread(image)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    undistorted = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    combined=thresholded_binary(undistorted)\n",
    "    warped,Minv = perspective_transform(combined)\n",
    "    \n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image', fontsize=50)\n",
    "    ax2.imshow(warped,'gray')\n",
    "    ax2.set_title('Perspective Transformed Image', fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Lane Boundary, Lane Curvature, and Vehicle Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "* After applying calibration, thresholding, and a perspective transform to a road image, we have a binary image where the lane lines stand out clearly. In order to decide explicitly which pixels are part of the lines and which belong to the left line and which belong to the right line, a histogram could be used. With this histogram I am adding up the pixel values along each column in the image. In my thresholded binary image, pixels are either 0 or 1, so the two most prominent peaks in this histogram will be good indicators of the x-position of the base of the lane lines. I can use that as a starting point for where to search for the lines. From that point, I can use a sliding window, placed around the line centers, to find and follow the lines up to the top of the frame. [The above methods are suggested by the course materials.]\n",
    "* Then, [Radius of Curvature](http://www.intmath.com/applications-differentiation/8-radius-curvature.php) could be computed. Note that I am fitting for $f(y)$, rather than $f(x)$, because the lane lines in the warped image are near vertical and may have the same $x$ value for more than one $y$ value. Besides, the radius of curvature may be given in meters.\n",
    "* In what follows, I computes left-lane-line, right-lane-line, and average them to obtain the center-line. Left-lane-line is `left_fitx`; right-lane-line is `right_fitx`; and the center-line is `avg_fitx`.\n",
    "* We can assume the camera is mounted at the center of the car and the deviation of the midpoint of the lane from the center of the image is the offset we're looking for. Therefore, offset could be obtained by the difference between the middle point of the image ($x$ direction) and the bottom end point of the center-line. Converting from pixels to meters, the offset is: `offset= (avg_fitx[-1]-640)*xm_per_pix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Find Lane Boundary, Lane Curvature, and Vehicle Position ####################\n",
    "def finding_lines(binary_warped): \n",
    "    # Assuming you have created a warped binary image called \"binary_warped\"\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]/2:,:], axis=0)\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.imshow(out_img)\n",
    "    #plt.plot(left_fitx, ploty, color='yellow')\n",
    "    #plt.plot(right_fitx, ploty, color='yellow')\n",
    "    #plt.xlim(0, 1280)\n",
    "    #plt.ylim(720, 0)\n",
    "    return out_img, left_fitx, right_fitx, ploty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curvature and offset along with the lane lines are shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = glob.glob('test_images/*.jpg')\n",
    "for image in images:\n",
    "    img = cv2.imread(image)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    undistorted = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    combined=thresholded_binary(undistorted)\n",
    "    warped, Minv = perspective_transform(combined)\n",
    "    out_img, left_fitx, right_fitx, ploty = finding_lines(warped)\n",
    "    avg_fitx=(left_fitx+right_fitx)/2\n",
    "    center_fit=np.polyfit(ploty, avg_fitx, 2)\n",
    "    #.....................................................................\n",
    "    # Define y-value where we want radius of curvature\n",
    "    # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    center_curverad = ((1 + (2*center_fit[0]*y_eval + center_fit[1])**2)**1.5) / np.absolute(2*center_fit[0])\n",
    "    #print(center_curverad)\n",
    "    # Example values: 1926.74 1908.48\n",
    "    #.....................................................................\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    center_fit_cr= np.polyfit(ploty*ym_per_pix, avg_fitx*xm_per_pix, 2)\n",
    "    # Calculate the new radii of curvature\n",
    "    center_curverad = ((1 + (2*center_fit_cr[0]*y_eval*ym_per_pix + center_fit_cr[1])**2)**1.5) / np.absolute(2*center_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "    print('Curvature= ',center_curverad, 'm')\n",
    "    # Example values: 632.1 m    626.2 m\n",
    "    #.....................................................................\n",
    "    offset= (avg_fitx[-1]-640)*xm_per_pix\n",
    "    print('Offset= ',offset,'m')\n",
    " \n",
    "    \n",
    "\n",
    "    #.....................................................................\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (img.shape[1], img.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undistorted, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    if offset >0:\n",
    "        side_pos='left'\n",
    "    elif offset <0:\n",
    "        side_pos='right'\n",
    "    else:\n",
    "        side_pos=' '\n",
    "\n",
    "    cv2.putText(result,'Radius of Curvature='+str(round(center_curverad,3))+'(m)',(50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    \n",
    "    cv2.putText(result,'Vehicle is '+str(abs(round(offset,3)))+'m '+side_pos+' of center',(50,100),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    plt.figure()\n",
    "    plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline: Drawing the Detected Lane on the Undistorted Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, I put all functions together, and plotted back down onto the road such that the lane area is identified clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    img = cv2.imread(image)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    undistorted = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    combined=thresholded_binary(undistorted)\n",
    "    warped, Minv = perspective_transform(combined)\n",
    "    out_img, left_fitx, right_fitx, ploty = finding_lines(warped)\n",
    "    avg_fitx=(left_fitx+right_fitx)/2\n",
    "    center_fit=np.polyfit(ploty, avg_fitx, 2)\n",
    "    #.....................................................................\n",
    "    # Define y-value where we want radius of curvature\n",
    "    # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    center_curverad = ((1 + (2*center_fit[0]*y_eval + center_fit[1])**2)**1.5) / np.absolute(2*center_fit[0])\n",
    "    #print(center_curverad)\n",
    "    # Example values: 1926.74 1908.48\n",
    "    #.....................................................................\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    center_fit_cr= np.polyfit(ploty*ym_per_pix, avg_fitx*xm_per_pix, 2)\n",
    "    # Calculate the new radii of curvature\n",
    "    center_curverad = ((1 + (2*center_fit_cr[0]*y_eval*ym_per_pix + center_fit_cr[1])**2)**1.5) / np.absolute(2*center_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "    print('Curvature= ',center_curverad, 'm')\n",
    "    # Example values: 632.1 m    626.2 m\n",
    "    #.....................................................................\n",
    "    offset= (avg_fitx[-1]-640)*xm_per_pix\n",
    "    print('Offset= ',offset,'m')\n",
    "    \n",
    "    #.....................................................................\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (img.shape[1], img.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undistorted, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    if offset >0:\n",
    "        side_pos='left'\n",
    "    elif offset <0:\n",
    "        side_pos='right'\n",
    "    else:\n",
    "        side_pos=' '\n",
    "\n",
    "    cv2.putText(result,'Radius of Curvature='+str(round(center_curverad,3))+'(m)',(50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    \n",
    "    cv2.putText(result,'Vehicle is '+str(abs(round(offset,3)))+'m '+side_pos+' of center',(50,100),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = glob.glob('test_images/*.jpg')\n",
    "for image in images:\n",
    "    result=process_image(image)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes are used for generating video file. You have to run the python code: `proj4.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "#from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "video_output = 'result0.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "video_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "video_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finally, the generated video file is `result.mp4`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion includes some consideration of problems/issues faced, what could be improved about their algorithm/pipeline, and what hypothetical cases would cause their pipeline to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most important problem I faced is that the pipeline cannot achieve real time computation. The length of the provided video clip is about 50 seconds. However, the processing time is about 20 minutes on my desktop. Recently, I would like to implement the pipeline to an embedded system, something like arduino or Raspberry Pi, but computational time will be a big problem.\n",
    "\n",
    "* Next, the threshold binary image creation plays an important role in the pipeline, but I feel the trial and error process is somewhat ad hoc. Maybe there are better systematic methods for obtaining good performance.\n",
    "* In first project, we use Hough Transform to detect a line, I wonder why we do not adopt or improve this method again in this project. I would like to try to adopt line detection algorithms in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
